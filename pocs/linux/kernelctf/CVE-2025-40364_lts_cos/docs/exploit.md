# Vulnerability
## Overview
Vulnerability is a use-after-free that exists in io_uring subsystem. It occurs when io_uring prepare for async operation for IO_READV. Prepared import provided buffers (`io_buffer_list` object) is not recycle after async preparation, which make it can be freed using `IORING_UNREGISTER_PBUF_RING` and will reused at the point where `IO_READV` executed asynchronously.

## Vuln Details
### io_uring Imported Buffer
IO Uring had a feature where we can register buffers that can used on most of io uring operation. Buffer stored in a `xarray`. We can register/unregister buffer using `IORING_REGISTER_PBUF_RING` or `IORING_UNREGISTER_PBUF_RING`. When issuing io_uring request, we can put the index of the buffer we want to use at `buf_index` and flag `IOSQE_BUFFER_SELECT` of io_uring submission context.
It will handled by `io_buffer_select` function.
```c
void __user *io_buffer_select(struct io_kiocb *req, size_t *len,
			      unsigned int issue_flags)
{
	struct io_ring_ctx *ctx = req->ctx;
	struct io_buffer_list *bl;
	void __user *ret = NULL;

	io_ring_submit_lock(req->ctx, issue_flags);

	bl = io_buffer_get_list(ctx, req->buf_index);
	if (likely(bl)) {
		if (bl->is_mapped)
			ret = io_ring_buffer_select(req, len, bl, issue_flags);
		else
			ret = io_provided_buffer_select(req, len, bl);
	}
	io_ring_submit_unlock(req->ctx, issue_flags);
	return ret;
}
```
There's two type of buffers, classic buffer and ring buffer, in this exploitation we gonna use ring buffer. `io_buffer_select` will call `io_ring_buffer_select` for our chosen buffer ring.
```c
static void __user *io_ring_buffer_select(struct io_kiocb *req, size_t *len,
					  struct io_buffer_list *bl,
					  unsigned int issue_flags)
{
	...
	req->flags |= REQ_F_BUFFER_RING;
	req->buf_list = bl;
	req->buf_index = buf->bid;
	...
}
```
Our buffer ring object will stored at `req->buf_list` here (without increase any refs at all), and will be used across io_uring operation later.

### io_uring Async Preparation
IO Uring had mechanism where it tries to execute operation asynchronously (which will execute in another kernel thread), before doing that io_uring uses `io_req_prep_async` to prepare some data context or object. Each operation can define `prep_async` by its own.
```C
int io_req_prep_async(struct io_kiocb *req)
{
	const struct io_cold_def *cdef = &io_cold_defs[req->opcode];
	const struct io_issue_def *def = &io_issue_defs[req->opcode];

	/* assign early for deferred execution for non-fixed file */
	if (def->needs_file && !(req->flags & REQ_F_FIXED_FILE) && !req->file)
		req->file = io_file_get_normal(req, req->cqe.fd);
	if (!cdef->prep_async)
		return 0;
	if (WARN_ON_ONCE(req_has_async_data(req)))
		return -EFAULT;
	if (!def->manual_alloc) {
		if (io_alloc_async_data(req))
			return -EAGAIN;
	}
	return cdef->prep_async(req);
}
```

### Problem
At IO_READV operation, `prep_async` will point to `io_readv_prep_async`. It then try to prepare our buffer ring via `io_import_iovec` -> `io_buffer_select`, at that point our `io_buffer_list` still stored at `req->buf_list` without any protection at all. After that we will unregister buffer ring using IORING_UNREGISTER_PBUF_RING, it will free the `io_buffer_list` at `io_put_bl`, because previously `req->buf_list` didn't hold any ref, at this code below it can smoothly free the `io_buffer_list`.
```c
void io_put_bl(struct io_ring_ctx *ctx, struct io_buffer_list *bl)
{
	if (atomic_dec_and_test(&bl->refs)) {
		__io_remove_buffers(ctx, bl, -1U);
		kfree_rcu(bl, rcu);
	}
}
```
After io_uring operation finish it will call `io_put_kbuf`, then it will call `__io_put_kbuf_list` for our buffer ring.
```c
static inline unsigned int __io_put_kbuf_list(struct io_kiocb *req,
					      struct list_head *list)
{
	unsigned int ret = IORING_CQE_F_BUFFER | (req->buf_index << IORING_CQE_BUFFER_SHIFT);

	if (req->flags & REQ_F_BUFFER_RING) {
		if (req->buf_list) {
			req->buf_index = req->buf_list->bgid;
			req->buf_list->head++; //[1]
		}
		req->flags &= ~REQ_F_BUFFER_RING;
	} else {
		req->buf_index = req->kbuf->bgid;
		list_add(&req->kbuf->list, list);
		req->flags &= ~REQ_F_BUFFER_SELECTED;
	}

	return ret;
}
```
At this point, `req->buf_list` still hold freed `io_buffer_list` then it will use-after-free at [1] when trying to increase the head. So we have use-after-free increase primitive.
